#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Feb 14 18:51:08 2022
modified 20221102 & 20240808
@author: dblaschke

PFDD -- Phase Field Dislocation Dynamics

Â© 2022. Triad National Security, LLC. All rights reserved.
This program was produced under U.S. Government contract 89233218CNA000001 for
Los Alamos National Laboratory (LANL), which is operated by Triad National
Security, LLC for the U.S. Department of Energy/National Nuclear Security
Administration. All rights in the program are reserved by Triad National
Security, LLC, and the U.S. Department of Energy/National Nuclear Security
Administration. The Government is granted for itself and others acting on its
behalf a nonexclusive, paid-up, irrevocable worldwide license in this material 
to reproduce, prepare derivative works, distribute copies to the public, perform
 publicly and display publicly, and to permit others to do so.
"""


import os
import sys
# from shutil import copyfile
# import glob
import numpy as np
import pandas as pd
import xarray as xr

try:
    from joblib import Parallel, delayed, cpu_count
    ## detect number of cpus present:
    Ncpus = cpu_count()
    ## choose how many cpu-cores are used for the parallelized calculations (also allowed: -1 = all available, -2 = all but one, etc.):
    Ncores = max(1,int(Ncpus/2)) ## don't overcommit, use half of the available cpus (on systems with hyperthreading this corresponds to the number of physical cpu cores)
    # Ncores = 1
except ImportError:
    print("WARNING: module 'joblib' not found, will run on only one core\n")
    Ncores = Ncpus = 1 ## must be 1 without joblib

from write_cond_inputfiles import approximateVf

def countlines(fname):
    '''returns the total number of lines for file 'fname'.'''
    with open(fname,"r", encoding="utf8") as inputfile:
        lines = inputfile.readlines()
        k = len(lines)
    return k

def str_to_number(arg):
    '''attemmpts to convert arg to int, if that fails we attempt conversion to float, and if that fails we return arg as is.'''
    try:
        out = int(arg)
    except (TypeError, ValueError):
        try:
            out = float(arg)
        except (TypeError, ValueError):
            out = arg
    return out

## fct loadinputfile: adapted from PyDislocDyn, see https://github.com/dblaschke-LANL/PyDislocDyn (bsd license)
def loadinputfile(fname):
    '''Reads an inputfile like the one generated by writeinputfile() defined in metal_data.py and returns its data as a dictionary.'''
    inputparams={'fname' : fname} ## remember the file we read from
    with open(fname,"r", encoding="utf8") as inputfile:
        lines = inputfile.readlines()
        for line in lines:
            if line[0] != "#":
                currentline = line.lstrip().rstrip().split()
                ## if spaces missing before/after '=', hack to match expected formatting:
                if len(currentline) ==1 and '=' in currentline[0]:
                    currentline = line.lstrip().rstrip().split('=')
                    currentline.insert(1,'=')
                if len(currentline) > 2:
                    key  = currentline[0]
                    if len(currentline)==3 or currentline[3]=='#':
                        value = currentline[2]
                        if value[-1] == '#':
                            value = str_to_number(value[:-1])
                        else:
                            value = str_to_number(value)
                    else:
                        value = [str_to_number(currentline[2])]
                        for i in range(len(currentline)-3):
                            addval = currentline[i+3]
                            if addval[0] == '#':
                                break
                            if value[-1] == '#':
                                value.append(str_to_number(value[:-1]))
                                break
                            value.append(str_to_number(addval))
                    inputparams[key] = value
    return inputparams

def read_conductivity(fname):
    '''searches file 'fname' for the last occurence of keyword 'conductivity' and returns its value'''
    conductivity = None
    try:
        with open(fname,"r", encoding="utf8") as inputfile:
            lines = inputfile.readlines()
            for line in lines:
                if 'conductivity' in line and not 'local' in line:
                    conductivity = line
    except IOError:
        pass
    if conductivity is None:
        out = np.nan ## only happens if pfdd code is still running and we want to postprocess some preliminary/incomplete data
    else:
        out = conductivity.split(":")
        if out[0]=='conductivity':
            out = float(out[1].lstrip().split(" ")[0])
    return out

if __name__ == '__main__':
    if len(sys.argv) >= 2:
        indata = loadinputfile(sys.argv[1])
    else:
        print(f"Usage: {sys.argv[0]} <inputfile.dat>\n (where the latter was created by 'write_cond_inputfile.py')"\
            "\naborting.")
        sys.exit()
        
    cwd = os.getcwd()
    fpath = indata['fname'].split('/')
    for i in fpath[:-1]:
        os.chdir(i)
    mater1 = indata['material1']
    mater2 = indata['material2']
    dlayer = np.linspace(*indata['dlayer'])*1e-9
    temperatures = np.linspace(*indata['temperatures'])
    Vf_target = np.linspace(*indata['Vf_target'])
    lattice_resolution = indata.get('lattice_resolution',128)
    Vf_and_dl = [] ## contains tuples of volume fraction and layer thickness relative to ref_layer thickness
    for vftar in Vf_target:
        Vf_and_dl.append(approximateVf(vftar, 1, lattice_resolution=lattice_resolution))
    Vf_effective = [vftar[0] for vftar in Vf_and_dl]
    threshold2g = 200 # old default
    if 'threshold2g' in indata:
        threshold2g = int(round(indata['threshold2g'])) ## in nm
    no_samples = 1000000
    stddev = indata.get('stddev',40) ## [nm]
    stddev_norm = stddev / ((indata['dlayer'][1]-indata['dlayer'][0])/(indata['dlayer'][2]-1))
    ##  =1.6 if stddev=40 with 25nm bins (default), need calculate stddev in terms of array indices
    gperc = indata.get('gperc',0.1)
    
    conductivity = np.zeros((len(temperatures),len(Vf_and_dl),len(dlayer)))
    conductivity_2g = np.zeros((len(temperatures),len(Vf_and_dl),len(dlayer)))
    for iT, T in enumerate(temperatures):
        Tfolder = f"T{T:.0f}"
        for iV,Vf in enumerate(Vf_and_dl):
            Vfolder = f"Vf{Vf[0]:.4f}"
            for i,dl in enumerate(dlayer):
                strdl = f"{1e9*dl:.0f}"
                fname = f"in{mater1}{mater2}_{strdl}"
                fpath = f"{Tfolder}/{Vfolder}/"
                conductivity[iT,iV,i] = read_conductivity(f"{fpath}{fname[2:]}.txt")
                if dl>=threshold2g*1e-9:
                    fname = f"in{mater1}{mater2}_{strdl}_g2"
                    fpath += "2g/"
                    conductivity_2g[iT,iV,i] = read_conductivity(f"{fpath}{fname[2:]}.txt")
                
    ## now that we have read all the raw data into 3D nunmpy arrays, we convert to xarray.DataArrays
    ## we also store conductivity in units of 10^7 S/m and layer thicknesses in nm within the latter:
    xrconduct = xr.DataArray(conductivity/1e7,name='conductivity [1e7 S/m]',dims=('T','Vf','dlayer'),\
                             coords={'T':temperatures,'Vf':Vf_effective,'dlayer':np.round(dlayer*1e9)})
    xrconduct_2g = xr.DataArray(conductivity_2g/1e7,name='conductivity [1e7 S/m]',dims=('T','Vf','dlayer'),\
                             coords={'T':temperatures,'Vf':Vf_effective,'dlayer':np.round(dlayer*1e9)})
    xrconduct_2g = xrconduct_2g.where(xrconduct_2g > 0)
    ## generate weighted data including 10% 2-grain results if dlayer is >=200nm:
    xrconductmix = 1.0*xrconduct
    xrconductmix.loc[:,:,threshold2g:] = (1-gperc)*xrconduct.loc[:,:,threshold2g:] + gperc*xrconduct_2g.loc[:,:,threshold2g:]
    
    ## account for layer thickness distribution
    ind1 = int(np.where(xrconductmix.coords['dlayer'].astype(int)==50)[0][0]) ## find index of 50nm
    ind2 = int(np.where(xrconductmix.coords['dlayer'].astype(int)==200)[0][0]) ## find index of 200nm
    layerrange = np.round(1e9*dlayer[ind1:ind2+1]).astype(int) ## only calculate for 50-200nm Cu layer thickness
    cond_wspread = 1.0*xrconductmix.loc[:,:,50:200]
    # cond_wspread.name = r'${\langle\sigma^\mathrm{if}\rangle_\mathrm{Gauss}}$'
    def calc_condwspread(iT):
        cond_wspread = 1.0*xrconductmix.loc[300,:,50:200] ## initialize same shape, values will be overwritten below
        for iV,Vf in enumerate(Vf_and_dl):
            for i,ind in enumerate(layerrange):
                currentidx = int(np.where(xrconductmix.coords['dlayer'].astype(int)==ind)[0][0])
                mask = np.clip(np.round(np.random.normal(loc=currentidx,scale=stddev_norm,size=no_samples)),0,len(dlayer)-1)
                cond_wspread[iV,i] = xrconductmix[iT,iV].to_pandas().iloc[mask].mean()
        return cond_wspread
    
    if Ncores == 1:
        cond_wspread_list = [calc_condwspread(iT) for iT in range(len(temperatures))]
    else:
        cond_wspread_list = Parallel(n_jobs=Ncores)(delayed(calc_condwspread)(iT) for iT in range(len(temperatures)))
    
    for iT, T in enumerate(temperatures):
        cond_wspread[iT] = cond_wspread_list[iT]
                
    ## combine results into one big dataarray:
    # cond_all = xr.concat([xrconduct,xrconductmix,cond_wspread],"results")
    # cond_all.coords["results"] = ['if','if+gb','if+gb+spread']
    
    ## write results to disk in various formats:
    fname = f"{mater1}{mater2}"
    ## write to netcdf file:
    # cond_all.to_netcdf(f'{fname}.nc')
    ##
    ## write csv files:
    # xrconduct.to_series().to_csv(f"{fname}_if.csv")
    # xrconduct_2g.to_series().to_csv(f"{fname}_2grains.csv")
    # xrconductmix.to_series().to_csv(f"{fname}_if+gb.csv")
    # cond_wspread.to_series().to_csv(f"{fname}_if+gb+spread.csv")
    ##
    ## write excel spreadsheet
    excelfile = pd.ExcelWriter(f"{fname}.xlsx")
    cond_wspread.to_series().to_excel(excelfile,sheet_name=f'{fname}, <if>+gb')
    xrconductmix.to_series().to_excel(excelfile,sheet_name=f'{fname}, if+{100*gperc:.0f}%gb>{threshold2g}')
    xrconduct.to_series().to_excel(excelfile,sheet_name=f'{fname}, if')
    xrconduct_2g.to_series().to_excel(excelfile,sheet_name=f'{fname}, 2grains')
    excelfile.close()
    ##
    ## write libreoffice spreadsheet (requires python module odfpy)
    # odsfile = pd.ExcelWriter(f"{fname}.ods")
    # cond_wspread.to_series().to_excel(odsfile,f'{fname}, <if>+gb')
    # xrconductmix.to_series().to_excel(odsfile,f'{fname}, if+10%gb>{threshold2g}')
    # xrconduct.to_series().to_excel(odsfile,f'{fname}, if')
    # xrconduct_2g.to_series().to_excel(odsfile,f'{fname}, 2grains')
    # odsfile.close()
    
    os.chdir(cwd) ## change back to where we started
    
    